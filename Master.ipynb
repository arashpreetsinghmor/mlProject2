{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#author: karanchauhan\n",
    "\"\"\"Generates a confusionMatrix\n",
    "Ideally number of distinct classes should be 6. 5 given class labels plus one class for the unknown\n",
    "Expects labels between 1-6. Preprocessing needs to be done to provide just matrices of class labels\"\"\"\n",
    "def ComputeConfusionMatrix(actualLabels, predictedLabels):\n",
    "    unique_number_of_class_labels = sorted(set(actualLabels))\n",
    "    matrix = np.zeros([len(unique_number_of_class_labels), len(unique_number_of_class_labels)], dtype=int)\n",
    "    for i in range(len(actualLabels)):\n",
    "        matrix[actualLabels[i]][predictedLabels[i]] += 1\n",
    "    confusion_matrix = matrix / matrix.sum(axis=1)[:, None]\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# author: karanchauhan\n",
    "#TODO Need to add hyperparameter tuning\n",
    "#TODO Improve performance\n",
    "def GPR(x_estimate, x_validate, class_labels):\n",
    "    # Instanciate a Gaussian Process model\n",
    "    print(\"started gpr\")\n",
    "    print(\"size of x_estimate: \" + str(len(x_estimate)))\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=1)\n",
    "    print(\"size of x_estimate: \" + str(len(x_estimate)))\n",
    "\n",
    "    #TODO n_restarts_optimizer should be higher\n",
    "\n",
    "    # Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "    gp.fit(x_estimate, class_labels)\n",
    "\n",
    "    # Make the prediction and assing values to class labels\n",
    "    y_validate, sigma = gp.predict(x_validate, return_std=True)\n",
    "    return y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Test\n",
    "# author: karanchauhan\n",
    "def CreateLabelsFromTestOutput(target_output, threshold):\n",
    "    classLabels = []\n",
    "    for i in target_output:\n",
    "        max_test_output = max(target_output[i])\n",
    "        max_test_output_index = target_output[i].index(max_test_output)\n",
    "        if(max_test_output_index>threshold):\n",
    "            classLabels[i] = max_test_output_index + 1\n",
    "        else: # Assign class label 6 for the unknown unclassifiable date\n",
    "            classLabels[i] = 6\n",
    "    return classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# author: karanchauhan\n",
    "def MyCrossValidate(XTrain, Nf, YTrain):\n",
    "\n",
    "    validation_set_size = len(XTrain)//Nf\n",
    "\n",
    "    print(len(XTrain) - validation_set_size)\n",
    "\n",
    "    # validation_sets is the Nf number of validation set (each of whose size is total data size/ number of folds\n",
    "    validation_sets = [[[0 for _ in range(Nf)] for _ in range(validation_set_size)] for _ in range(len(XTrain[0]))]\n",
    "\n",
    "    # actual_validation_output is the actual training output corresponding to the validation set\n",
    "    actual_validation_output = [[[0 for _ in range(Nf)] for _ in range(validation_set_size)] for _ in range(len(XTrain[0]))]\n",
    "\n",
    "    # threshold set for classifying a test data sample to class 6 if any of models give all class values<threshold\n",
    "    threshold = 0.7\n",
    "\n",
    "    # estimation_sets is the Nf number of estimation sets (each of which contains total data - size of validation set)\n",
    "    estimation_sets = [[[0 for _ in range(Nf)] for _ in range(len(XTrain)-validation_set_size)] for _ in range(len(XTrain[0]))]\n",
    "\n",
    "    # y_train_sets is the Nf number of actual training output corresponding to each estimation set\n",
    "    y_train_sets = [[[0 for _ in range(Nf)] for _ in range(len(XTrain)-validation_set_size)] for _ in range(len(YTrain[0]))]\n",
    "\n",
    "    # Shuffle the data set and label set for random partition\n",
    "    c = list(zip(XTrain, YTrain))\n",
    "    random.shuffle(c)\n",
    "    XTrain, YTrain = zip(*c)\n",
    "\n",
    "    # Partition and store in partionedLabelSet\n",
    "    x = 0\n",
    "    for i in range(Nf):\n",
    "\n",
    "        validation_sets[i] = XTrain[x:x+validation_set_size]\n",
    "        actual_validation_output[i] = YTrain[x:x + validation_set_size]\n",
    "\n",
    "        indices_to_ignore = range(x, x+validation_set_size)\n",
    "        estimation_sets[i] = [estimation_set for index, estimation_set in enumerate(XTrain) if index not in (indices_to_ignore)]\n",
    "\n",
    "        y_train_sets[i] = [i for j, i in enumerate(YTrain) if j not in indices_to_ignore]\n",
    "\n",
    "        x += validation_set_size\n",
    "\n",
    "    # Run for all validation and estimation sets\n",
    "    for i in range(Nf):\n",
    "        #TODO Call GPR, SVM, RVM\n",
    "        y_validate = GPR(estimation_sets[i],validation_sets[i], y_train_sets[i])\n",
    "\n",
    "        # actual_class_labels are the class labels (1-6) defined for the actual_validation_output\n",
    "        actual_class_labels = CreateLabelsFromTestOutput(actual_validation_output[i])\n",
    "\n",
    "        # predicted_class_labels are the class labels (1-6) defined for the predicted validation output\n",
    "        predicted_class_labels = CreateLabelsFromTestOutput(y_validate)\n",
    "\n",
    "        #Compute confusion matrix (actual_class_labels, predicted_class_labels)\n",
    "        confusion_matrix = ComputeConfusionMatrix(actual_class_labels, predicted_class_labels)\n",
    "\n",
    "        #TODO Compute average confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# author: Jianing\n",
    "# def SVMTraining(XEstimate,XValidate,Parameters):\n",
    "#     clf = svm.SVC(decision_function_shape='ovo')\n",
    "#     print clf.fit(XEstimate, XValidate)\n",
    "#     Yvalidate=clf.predict(Parameters)\n",
    "#     EstParameters=clf.get_params()\n",
    "#     return {\"Yvalidate\": Yvalidate,\n",
    "#             \"EstParameters\": EstParameters}\n",
    "\n",
    "# Path of training set and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load image data\n",
    "loadFeatVecsMatData = sio.loadmat(os.getcwd() + '/Proj2FeatVecsSet1.mat')\n",
    "loadLabelsMatData = sio.loadmat(os.getcwd() + '/Proj2TargetOutputsSet1.mat')\n",
    "\n",
    "trainingSet = loadFeatVecsMatData['Proj2FeatVecsSet1']\n",
    "labelSet = loadLabelsMatData['Proj2TargetOutputsSet1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started gpr\n",
      "size of x_estimate: 5\n",
      "size of x_estimate: 5\n",
      "[[ 1.00000359 -1.00000359 -1.00000359 -1.00000359 -1.00000359]\n",
      " [ 0.99999939 -0.99999939 -0.99999939 -0.99999939 -0.99999939]\n",
      " [ 1.00000015 -1.00000015 -1.00000015 -1.00000015 -1.00000015]\n",
      " [ 0.99998071 -0.99998071 -0.99998071 -0.99998071 -0.99998071]\n",
      " [ 1.00000188 -1.00000188 -1.00000188 -1.00000188 -1.00000188]]\n",
      "[[ 1 -1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1 -1]\n",
      " [ 1 -1 -1 -1 -1]]\n",
      "16667\n",
      "started gpr\n",
      "size of x_estimate: 16667\n",
      "size of x_estimate: 16667\n"
     ]
    }
   ],
   "source": [
    "# Running GPR for just 5 samples\n",
    "print(GPR(trainingSet[0:5], trainingSet[6:11], labelSet[0:5]))\n",
    "print(labelSet[6:11])\n",
    "\n",
    "MyCrossValidate(trainingSet, 3, labelSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
